{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "import numpy as np\n",
    "import os\n",
    "from os import listdir\n",
    "from keras.layers import Input, Dense, LSTM, Dropout, Embedding, add\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import glob\n",
    "from numpy import array\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read captions corresponding to each image and store them in list\n",
    "filename = \"flicker8k-dataset/Flickr8k_text/Flickr8k.token.txt\"\n",
    "file = open(filename, 'r')\n",
    "doc = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the captions file. Process each line extract 5 captions for each image and append to list.\n",
    "descriptions = dict()\n",
    "for line in doc.split('\\n'):\n",
    "    # Splitting the line by tab space\n",
    "    tokens = line.split('\\t')\n",
    "    # Storing image id and descriptions in different variables\n",
    "    image_id, image_desc = tokens[0], tokens[1:]\n",
    "    # Removing the extension of image type from the image id\n",
    "    image_id = image_id.split('.')[0]\n",
    "    # Storing all the descriptions as one string\n",
    "    image_desc = ' '.join(image_desc)\n",
    "    if image_id not in descriptions:\n",
    "        descriptions[image_id] = list()\n",
    "    descriptions[image_id].append(image_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the image captions\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "for key, desc_list in descriptions.items():\n",
    "    for i in range(len(desc_list)):\n",
    "        desc = desc_list[i]\n",
    "        # Tokenizing the string\n",
    "        desc = desc.split()\n",
    "        # Converting the entire string to lower case\n",
    "        desc = [word.lower() for word in desc]\n",
    "        # Removing punctuation from each token\n",
    "        desc = [w.translate(table) for w in desc]\n",
    "        # Removing 's and article \"A\"\n",
    "        desc = [word for word in desc if len(word)>1]\n",
    "        # Removing words with numbers\n",
    "        desc = [word for word in desc if word.isalpha()]\n",
    "        # Storing the caption as a string\n",
    "        desc_list[i] =  ' '.join(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Vocabulary Size: 8763\n"
     ]
    }
   ],
   "source": [
    "# Creating an empty set for vocabulary to store unique words\n",
    "vocabulary = set()\n",
    "# Counting the size of vocabulary\n",
    "for key in descriptions.keys():\n",
    "    [vocabulary.update(d.split()) for d in descriptions[key]]\n",
    "\n",
    "print('Original Vocabulary Size: %d' % len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below path contains all the images\n",
    "images = 'flicker8k-dataset/Flickr8k_Dataset/Flicker8k_Dataset/'\n",
    "# Creating a list of all the image names in the directory\n",
    "img = glob.glob(images + '*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below file conatains the names of images to be used in train data\n",
    "train_images_file = 'flicker8k-dataset/Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "# Reading the train image names in a set\n",
    "train_images = set(open(train_images_file, 'r').read().strip().split('\\n'))\n",
    "\n",
    "# Creating a list of all the training images with their full path names\n",
    "train_img = []\n",
    "\n",
    "for i in img: # img contains full path names of all images\n",
    "    if i[len(images):] in train_images: # Checking if the image belongs to training set\n",
    "        train_img.append(i) # Adding it to the list of train images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Development set same as above\n",
    "dev_images_file = 'flicker8k-dataset/Flickr8k_text/Flickr_8k.devImages.txt'\n",
    "dev_images = set(open(dev_images_file, 'r').read().strip().split('\\n'))\n",
    "\n",
    "dev_img = []\n",
    "\n",
    "for i in img: \n",
    "    if i[len(images):] in dev_images: \n",
    "        dev_img.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Test set same as above\n",
    "test_images_file = 'flicker8k-dataset/Flickr8k_text/Flickr_8k.testImages.txt'\n",
    "test_images = set(open(test_images_file, 'r').read().strip().split('\\n'))\n",
    "\n",
    "test_img = []\n",
    "\n",
    "for i in img:\n",
    "    if i[len(images):] in test_images:\n",
    "        test_img.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1000268201_693b08cb0e', ['child in pink dress is climbing up set of stairs in an entry way', 'girl going into wooden building', 'little girl climbing into wooden playhouse', 'little girl climbing the stairs to her playhouse', 'little girl in pink dress going into wooden cabin']], ['1001773457_577c3a7d70', ['black dog and spotted dog are fighting', 'black dog and tricolored dog playing with each other on the road', 'black dog and white dog with brown spots are staring at each other in the street', 'two dogs of different breeds looking at each other on the road', 'two dogs on pavement moving toward each other']]]\n"
     ]
    }
   ],
   "source": [
    "# creating list to store image and corresponding 5 captions\n",
    "def caption_dataset(data):\n",
    "    desc = list()\n",
    "    for key, value in descriptions.items():\n",
    "        temp = [key,value]\n",
    "        if key+'.jpg' in data:\n",
    "            desc.append(temp)\n",
    "    return desc\n",
    "\n",
    "train_desc = caption_dataset(train_images) #list\n",
    "dev_desc = caption_dataset(dev_images)\n",
    "test_desc = caption_dataset(test_images)\n",
    "print(train_desc[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert above generated list to dictionary for faster accessing\n",
    "train_description,dev_description,test_description= {},{},{}\n",
    "for each in train_desc:\n",
    "    train_description[each[0]] = each[1]\n",
    "for each in dev_desc:\n",
    "    dev_description[each[0]] = each[1]\n",
    "for each in test_desc:\n",
    "    test_description[each[0]] = each[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_size(description):\n",
    "    # Create a list of all the training captions\n",
    "    all_captions = []\n",
    "    for key, val in description.items():\n",
    "        for cap in val:\n",
    "            all_captions.append(cap)\n",
    "\n",
    "\n",
    "    # Consider only words which occur at least 10 times in the corpus\n",
    "    word_count_threshold = 10\n",
    "    word_counts = {}\n",
    "    nsents = 0\n",
    "    for sent in all_captions:\n",
    "        nsents += 1\n",
    "        for w in sent.split(' '):\n",
    "            word_counts[w] = word_counts.get(w, 0) + 1\n",
    "\n",
    "    vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
    "\n",
    "    print('preprocessed words %d ' % len(vocab))\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['start child in pink dress is climbing up set of stairs in an entry way end', 'start girl going into wooden building end', 'start little girl climbing into wooden playhouse end', 'start little girl climbing the stairs to her playhouse end', 'start little girl in pink dress going into wooden cabin end']\n"
     ]
    }
   ],
   "source": [
    "# Adding 'start' and 'end' to all captions just to identify start and end of sentence \n",
    "temp = []\n",
    "for key, value in train_description.items():\n",
    "    temp = []\n",
    "    for each in value:\n",
    "        str1 = 'start '+ each+' end'\n",
    "        temp.append(str1)\n",
    "    train_description[key] = temp\n",
    "print(train_description['1000268201_693b08cb0e'])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding 'start' and 'end' to all captions just to identify start and end of sentence in dev data set\n",
    "temp = []\n",
    "for key, value in dev_description.items():\n",
    "    temp = []\n",
    "    for each in value:\n",
    "        str1 = 'start '+ each+' end'\n",
    "        temp.append(str1)\n",
    "    dev_description[key] = temp\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels.h5\n",
      "96116736/96112376 [==============================] - 9s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Using pretrained InceptionV3 model trained on imagenet data\n",
    "model = InceptionV3(weights='imagenet')\n",
    "# Removing the last layer (output softmax layer)\n",
    "model_new = Model(model.input, model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting all the images to size 299x299 as expected by inception v3 model\n",
    "\n",
    "train_features={}\n",
    "for img in train_img:\n",
    "        # load an image from file\n",
    "        image = load_img(img, target_size=(299, 299))\n",
    "        # convert the image pixels to a numpy array\n",
    "        x = img_to_array(image)\n",
    "        # Add one more dimension\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        # preprocess images using preprocess_input() from inception module\n",
    "        x = preprocess_input(x)\n",
    "        x = model_new.predict(x, verbose=0)\n",
    "        feature = np.reshape(x, x.shape[1])\n",
    "        train_features[img[len(images):]] = feature\n",
    "    \n",
    "\n",
    "print(len(train_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev features\n",
    "dev_features={}\n",
    "for img in dev_img:\n",
    "        # load an image from file\n",
    "        image = load_img(img, target_size=(299, 299))\n",
    "        # convert the image pixels to a numpy array\n",
    "        x = img_to_array(image)\n",
    "        # Add one more dimension\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        # preprocess images using preprocess_input() from inception module\n",
    "        x = preprocess_input(x)\n",
    "        x = model_new.predict(x, verbose=0)\n",
    "        feature = np.reshape(x, x.shape[1])\n",
    "        dev_features[img[len(images):]] = feature\n",
    "    \n",
    "\n",
    "print(len(dev_features),type(dev_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating indexes for all words in vocabulary\n",
    "def word_ix_fun(vocab):\n",
    "    ixtoword = {}\n",
    "    wordtoix = {}\n",
    "    ix = 1\n",
    "    for w in vocab:\n",
    "        wordtoix[w] = ix\n",
    "        ixtoword[ix] = w\n",
    "        ix += 1\n",
    "\n",
    "    print(len(ixtoword))\n",
    "    return wordtoix,ixtoword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting dictionary to list\n",
    "def dict_list(set1,set2):\n",
    "    set0,set0_list={},[]\n",
    "    set0.update(set1)\n",
    "    set0.update(set2)\n",
    "\n",
    "    for key, value in set0.items():\n",
    "        temp = [key,value]\n",
    "        set0_list.append(temp)\n",
    "    return set0_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Feature list and Dictionary list\n",
    "#feature_list = dict_list(train_features,dev_features)\n",
    "description_list = dict_list(train_description,dev_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Description Length: 34\n"
     ]
    }
   ],
   "source": [
    "# converting a dictionary of clean descriptions to a list of descriptions\n",
    "def to_lines(descriptions):\n",
    "    all_desc = list()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.append(d) for d in descriptions[key]]\n",
    "    return all_desc\n",
    "\n",
    "# calculating the length of the descriptions with the most words\n",
    "def max_length(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    return max(len(d.split()) for d in lines)\n",
    "\n",
    "# determining the maximum sequence length\n",
    "max_length = max(max_length(train_description),max_length(dev_description))\n",
    "print('Max Description Length: %d' % max_length)\n",
    "# Max Description Length: 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generator, intended to be used in a call to model.fit_generator()\n",
    "def data_generator(descriptions, photos, wordtoix,max_length, num_photos_per_batch):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    n=0\n",
    "    \n",
    "    # loop for ever over images\n",
    "    while 1:\n",
    "        for key, desc_list in descriptions.items():\n",
    "            n+=1\n",
    "            # retrieve the photo feature\n",
    "            photo = photos[key]\n",
    "            for desc in desc_list:\n",
    "                # encode the sequence\n",
    "                seq = [wordtoix[word] for word in desc.split(' ') if word in wordtoix]\n",
    "                # split one sequence into multiple X, y pairs\n",
    "                for i in range(1, len(seq)):\n",
    "                    # split into input and output pair\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    # pad input sequence\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                    # encode output sequence\n",
    "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                    # store\n",
    "                    X1.append(photo)\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "            # yield the batch data\n",
    "            if n==num_photos_per_batch:\n",
    "                yield [[array(X1), array(X2)], array(y)]\n",
    "                X1, X2, y = list(), list(), list()\n",
    "                n=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Glove vectors\n",
    "glove_dir = 'Glove/'\n",
    "embeddings_index = {} # empty dictionary\n",
    "vocab_size=10000\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.200d.txt'), encoding=\"utf-8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Glove embeddings 200 dim dense vector for our vocabulary\n",
    "def data_weight(wordtoix):\n",
    "    embedding_dim = 200\n",
    "    vocab_size = 10000\n",
    "    # Get 200-dim dense vector for each of the 10000 words in out vocabulary\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    for word, i in wordtoix.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in the embedding index will be all zeros\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    print(len(embedding_matrix[0]))\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image feature extractor model\n",
    "inputs1 = Input(shape=(2048,))\n",
    "fe1 = Dropout(0.5)(inputs1)\n",
    "fe2 = Dense(256, activation='relu')(fe1)\n",
    "\n",
    "# partial caption sequence model\n",
    "inputs2 = Input(shape=(max_length,))\n",
    "se1 = Embedding(vocab_size, 200, mask_zero=True)(inputs2)\n",
    "se2 = Dropout(0.5)(se1)\n",
    "se3 = LSTM(256)(se2)\n",
    "\n",
    "# decoder (feed forward) model\n",
    "decoder1 = add([fe2, se3])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\n",
    "# merge the two input models\n",
    "model = Model(inputs=[inputs1, inputs2], outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 34)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 2048)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 34, 200)      2000000     input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 2048)         0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 34, 200)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          524544      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 256)          467968      dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 256)          0           dense_2[0][0]                    \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          65792       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 10000)        2570000     dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,628,304\n",
      "Trainable params: 5,628,304\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
