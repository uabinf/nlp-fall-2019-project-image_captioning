{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "import numpy as np\n",
    "import os\n",
    "from os import listdir\n",
    "from keras.layers import Input, Dense, LSTM, Dropout, Embedding, add\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import glob\n",
    "from numpy import array\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read captions corresponding to each image and store them in list\n",
    "filename = \"flicker8k-dataset/Flickr8k_text/Flickr8k.token.txt\"\n",
    "file = open(filename, 'r')\n",
    "doc = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the captions file. Process each line extract 5 captions for each image and append to list.\n",
    "descriptions = dict()\n",
    "for line in doc.split('\\n'):\n",
    "    # Splitting the line by tab space\n",
    "    tokens = line.split('\\t')\n",
    "    # Storing image id and descriptions in different variables\n",
    "    image_id, image_desc = tokens[0], tokens[1:]\n",
    "    # Removing the extension of image type from the image id\n",
    "    image_id = image_id.split('.')[0]\n",
    "    # Storing all the descriptions as one string\n",
    "    image_desc = ' '.join(image_desc)\n",
    "    if image_id not in descriptions:\n",
    "        descriptions[image_id] = list()\n",
    "    descriptions[image_id].append(image_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the image captions\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "for key, desc_list in descriptions.items():\n",
    "    for i in range(len(desc_list)):\n",
    "        desc = desc_list[i]\n",
    "        # Tokenizing the string\n",
    "        desc = desc.split()\n",
    "        # Converting the entire string to lower case\n",
    "        desc = [word.lower() for word in desc]\n",
    "        # Removing punctuation from each token\n",
    "        desc = [w.translate(table) for w in desc]\n",
    "        # Removing 's and article \"A\"\n",
    "        desc = [word for word in desc if len(word)>1]\n",
    "        # Removing words with numbers\n",
    "        desc = [word for word in desc if word.isalpha()]\n",
    "        # Storing the caption as a string\n",
    "        desc_list[i] =  ' '.join(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Vocabulary Size: 8763\n"
     ]
    }
   ],
   "source": [
    "# Creating an empty set for vocabulary to store unique words\n",
    "vocabulary = set()\n",
    "# Counting the size of vocabulary\n",
    "for key in descriptions.keys():\n",
    "    [vocabulary.update(d.split()) for d in descriptions[key]]\n",
    "\n",
    "print('Original Vocabulary Size: %d' % len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below path contains all the images\n",
    "images = 'flicker8k-dataset/Flickr8k_Dataset/Flicker8k_Dataset/'\n",
    "# Creating a list of all the image names in the directory\n",
    "img = glob.glob(images + '*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below file conatains the names of images to be used in train data\n",
    "train_images_file = 'flicker8k-dataset/Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "# Reading the train image names in a set\n",
    "train_images = set(open(train_images_file, 'r').read().strip().split('\\n'))\n",
    "\n",
    "# Creating a list of all the training images with their full path names\n",
    "train_img = []\n",
    "\n",
    "for i in img: # img contains full path names of all images\n",
    "    if i[len(images):] in train_images: # Checking if the image belongs to training set\n",
    "        train_img.append(i) # Adding it to the list of train images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Development set same as above\n",
    "dev_images_file = 'flicker8k-dataset/Flickr8k_text/Flickr_8k.devImages.txt'\n",
    "dev_images = set(open(dev_images_file, 'r').read().strip().split('\\n'))\n",
    "\n",
    "dev_img = []\n",
    "\n",
    "for i in img: \n",
    "    if i[len(images):] in dev_images: \n",
    "        dev_img.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Test set same as above\n",
    "test_images_file = 'flicker8k-dataset/Flickr8k_text/Flickr_8k.testImages.txt'\n",
    "test_images = set(open(test_images_file, 'r').read().strip().split('\\n'))\n",
    "\n",
    "test_img = []\n",
    "\n",
    "for i in img:\n",
    "    if i[len(images):] in test_images:\n",
    "        test_img.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1000268201_693b08cb0e', ['child in pink dress is climbing up set of stairs in an entry way', 'girl going into wooden building', 'little girl climbing into wooden playhouse', 'little girl climbing the stairs to her playhouse', 'little girl in pink dress going into wooden cabin']], ['1001773457_577c3a7d70', ['black dog and spotted dog are fighting', 'black dog and tricolored dog playing with each other on the road', 'black dog and white dog with brown spots are staring at each other in the street', 'two dogs of different breeds looking at each other on the road', 'two dogs on pavement moving toward each other']]]\n"
     ]
    }
   ],
   "source": [
    "# creating list to store image and corresponding 5 captions\n",
    "def caption_dataset(data):\n",
    "    desc = list()\n",
    "    for key, value in descriptions.items():\n",
    "        temp = [key,value]\n",
    "        if key+'.jpg' in data:\n",
    "            desc.append(temp)\n",
    "    return desc\n",
    "\n",
    "train_desc = caption_dataset(train_images) #list\n",
    "dev_desc = caption_dataset(dev_images)\n",
    "test_desc = caption_dataset(test_images)\n",
    "print(train_desc[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert above generated list to dictionary for faster accessing\n",
    "train_description,dev_description,test_description= {},{},{}\n",
    "for each in train_desc:\n",
    "    train_description[each[0]] = each[1]\n",
    "for each in dev_desc:\n",
    "    dev_description[each[0]] = each[1]\n",
    "for each in test_desc:\n",
    "    test_description[each[0]] = each[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_size(description):\n",
    "    # Create a list of all the training captions\n",
    "    all_captions = []\n",
    "    for key, val in description.items():\n",
    "        for cap in val:\n",
    "            all_captions.append(cap)\n",
    "\n",
    "\n",
    "    # Consider only words which occur at least 10 times in the corpus\n",
    "    word_count_threshold = 10\n",
    "    word_counts = {}\n",
    "    nsents = 0\n",
    "    for sent in all_captions:\n",
    "        nsents += 1\n",
    "        for w in sent.split(' '):\n",
    "            word_counts[w] = word_counts.get(w, 0) + 1\n",
    "\n",
    "    vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
    "\n",
    "    print('preprocessed words %d ' % len(vocab))\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['start child in pink dress is climbing up set of stairs in an entry way end', 'start girl going into wooden building end', 'start little girl climbing into wooden playhouse end', 'start little girl climbing the stairs to her playhouse end', 'start little girl in pink dress going into wooden cabin end']\n"
     ]
    }
   ],
   "source": [
    "# Adding 'start' and 'end' to all captions just to identify start and end of sentence \n",
    "temp = []\n",
    "for key, value in train_description.items():\n",
    "    temp = []\n",
    "    for each in value:\n",
    "        str1 = 'start '+ each+' end'\n",
    "        temp.append(str1)\n",
    "    train_description[key] = temp\n",
    "print(train_description['1000268201_693b08cb0e'])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding 'start' and 'end' to all captions just to identify start and end of sentence in dev data set\n",
    "temp = []\n",
    "for key, value in dev_description.items():\n",
    "    temp = []\n",
    "    for each in value:\n",
    "        str1 = 'start '+ each+' end'\n",
    "        temp.append(str1)\n",
    "    dev_description[key] = temp\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels.h5\n",
      "96116736/96112376 [==============================] - 9s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Using pretrained InceptionV3 model trained on imagenet data\n",
    "model = InceptionV3(weights='imagenet')\n",
    "# Removing the last layer (output softmax layer)\n",
    "model_new = Model(model.input, model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting all the images to size 299x299 as expected by inception v3 model\n",
    "\n",
    "train_features={}\n",
    "for img in train_img:\n",
    "        # load an image from file\n",
    "        image = load_img(img, target_size=(299, 299))\n",
    "        # convert the image pixels to a numpy array\n",
    "        x = img_to_array(image)\n",
    "        # Add one more dimension\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        # preprocess images using preprocess_input() from inception module\n",
    "        x = preprocess_input(x)\n",
    "        x = model_new.predict(x, verbose=0)\n",
    "        feature = np.reshape(x, x.shape[1])\n",
    "        train_features[img[len(images):]] = feature\n",
    "    \n",
    "\n",
    "print(len(train_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev features\n",
    "dev_features={}\n",
    "for img in dev_img:\n",
    "        # load an image from file\n",
    "        image = load_img(img, target_size=(299, 299))\n",
    "        # convert the image pixels to a numpy array\n",
    "        x = img_to_array(image)\n",
    "        # Add one more dimension\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        # preprocess images using preprocess_input() from inception module\n",
    "        x = preprocess_input(x)\n",
    "        x = model_new.predict(x, verbose=0)\n",
    "        feature = np.reshape(x, x.shape[1])\n",
    "        dev_features[img[len(images):]] = feature\n",
    "    \n",
    "\n",
    "print(len(dev_features),type(dev_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
