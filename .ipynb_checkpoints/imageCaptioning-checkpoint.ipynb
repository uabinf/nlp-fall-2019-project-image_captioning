{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "import numpy as np\n",
    "import os\n",
    "from os import listdir\n",
    "from keras.layers import Input, Dense, LSTM, Dropout, Embedding, add\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import glob\n",
    "from numpy import array\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read captions corresponding to each image and store them in list\n",
    "filename = \"flicker8k-dataset/Flickr8k_text/Flickr8k.token.txt\"\n",
    "file = open(filename, 'r')\n",
    "doc = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the captions file. Process each line extract 5 captions for each image and append to list.\n",
    "descriptions = dict()\n",
    "for line in doc.split('\\n'):\n",
    "    # Splitting the line by tab space\n",
    "    tokens = line.split('\\t')\n",
    "    # Storing image id and descriptions in different variables\n",
    "    image_id, image_desc = tokens[0], tokens[1:]\n",
    "    # Removing the extension of image type from the image id\n",
    "    image_id = image_id.split('.')[0]\n",
    "    # Storing all the descriptions as one string\n",
    "    image_desc = ' '.join(image_desc)\n",
    "    if image_id not in descriptions:\n",
    "        descriptions[image_id] = list()\n",
    "    descriptions[image_id].append(image_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the image captions\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "for key, desc_list in descriptions.items():\n",
    "    for i in range(len(desc_list)):\n",
    "        desc = desc_list[i]\n",
    "        # Tokenizing the string\n",
    "        desc = desc.split()\n",
    "        # Converting the entire string to lower case\n",
    "        desc = [word.lower() for word in desc]\n",
    "        # Removing punctuation from each token\n",
    "        desc = [w.translate(table) for w in desc]\n",
    "        # Removing 's and article \"A\"\n",
    "        desc = [word for word in desc if len(word)>1]\n",
    "        # Removing words with numbers\n",
    "        desc = [word for word in desc if word.isalpha()]\n",
    "        # Storing the caption as a string\n",
    "        desc_list[i] =  ' '.join(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Vocabulary Size: 8763\n"
     ]
    }
   ],
   "source": [
    "# Creating an empty set for vocabulary to store unique words\n",
    "vocabulary = set()\n",
    "# Counting the size of vocabulary\n",
    "for key in descriptions.keys():\n",
    "    [vocabulary.update(d.split()) for d in descriptions[key]]\n",
    "\n",
    "print('Original Vocabulary Size: %d' % len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below path contains all the images\n",
    "images = 'flicker8k-dataset/Flickr8k_Dataset/Flicker8k_Dataset/'\n",
    "# Creating a list of all the image names in the directory\n",
    "img = glob.glob(images + '*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below file conatains the names of images to be used in train data\n",
    "train_images_file = 'flicker8k-dataset/Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "# Reading the train image names in a set\n",
    "train_images = set(open(train_images_file, 'r').read().strip().split('\\n'))\n",
    "\n",
    "# Creating a list of all the training images with their full path names\n",
    "train_img = []\n",
    "\n",
    "for i in img: # img contains full path names of all images\n",
    "    if i[len(images):] in train_images: # Checking if the image belongs to training set\n",
    "        train_img.append(i) # Adding it to the list of train images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Development set same as above\n",
    "dev_images_file = 'flicker8k-dataset/Flickr8k_text/Flickr_8k.devImages.txt'\n",
    "dev_images = set(open(dev_images_file, 'r').read().strip().split('\\n'))\n",
    "\n",
    "dev_img = []\n",
    "\n",
    "for i in img: \n",
    "    if i[len(images):] in dev_images: \n",
    "        dev_img.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Test set same as above\n",
    "test_images_file = 'flicker8k-dataset/Flickr8k_text/Flickr_8k.testImages.txt'\n",
    "test_images = set(open(test_images_file, 'r').read().strip().split('\\n'))\n",
    "\n",
    "test_img = []\n",
    "\n",
    "for i in img:\n",
    "    if i[len(images):] in test_images:\n",
    "        test_img.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1000268201_693b08cb0e', ['child in pink dress is climbing up set of stairs in an entry way', 'girl going into wooden building', 'little girl climbing into wooden playhouse', 'little girl climbing the stairs to her playhouse', 'little girl in pink dress going into wooden cabin']], ['1001773457_577c3a7d70', ['black dog and spotted dog are fighting', 'black dog and tricolored dog playing with each other on the road', 'black dog and white dog with brown spots are staring at each other in the street', 'two dogs of different breeds looking at each other on the road', 'two dogs on pavement moving toward each other']]]\n"
     ]
    }
   ],
   "source": [
    "# creating list to store image and corresponding 5 captions\n",
    "def caption_dataset(data):\n",
    "    desc = list()\n",
    "    for key, value in descriptions.items():\n",
    "        temp = [key,value]\n",
    "        if key+'.jpg' in data:\n",
    "            desc.append(temp)\n",
    "    return desc\n",
    "\n",
    "train_desc = caption_dataset(train_images) #list\n",
    "dev_desc = caption_dataset(dev_images)\n",
    "test_desc = caption_dataset(test_images)\n",
    "print(train_desc[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert above generated list to dictionary for faster accessing\n",
    "train_description,dev_description,test_description= {},{},{}\n",
    "for each in train_desc:\n",
    "    train_description[each[0]] = each[1]\n",
    "for each in dev_desc:\n",
    "    dev_description[each[0]] = each[1]\n",
    "for each in test_desc:\n",
    "    test_description[each[0]] = each[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_size(description):\n",
    "    # Create a list of all the training captions\n",
    "    all_captions = []\n",
    "    for key, val in description.items():\n",
    "        for cap in val:\n",
    "            all_captions.append(cap)\n",
    "\n",
    "\n",
    "    # Consider only words which occur at least 10 times in the corpus\n",
    "    word_count_threshold = 10\n",
    "    word_counts = {}\n",
    "    nsents = 0\n",
    "    for sent in all_captions:\n",
    "        nsents += 1\n",
    "        for w in sent.split(' '):\n",
    "            word_counts[w] = word_counts.get(w, 0) + 1\n",
    "\n",
    "    vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
    "\n",
    "    print('preprocessed words %d ' % len(vocab))\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
